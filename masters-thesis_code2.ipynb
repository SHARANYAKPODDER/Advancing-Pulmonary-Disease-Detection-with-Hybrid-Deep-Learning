{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9217566,"sourceType":"datasetVersion","datasetId":5574031}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nfrom tqdm import tqdm\n\n\nbase_path = '/kaggle/input/sounddata' \n\n\nnpy_destination = '/kaggle/working/collected_preprocessed_train'\nlabel_destination = '/kaggle/working/collected_labels'\n\nos.makedirs(npy_destination, exist_ok=True)\nos.makedirs(label_destination, exist_ok=True)\n\nnpy_files_count = 0\nlabel_files_count = 0\n\n# Loop through each directory (001 to 006)\nfor i in range(1, 7):\n    folder_name = f'HF_Lung_V1-20240819T223709Z-00{i}'\n    preprocessed_train_path = os.path.join(base_path, folder_name, 'HF_Lung_V1/preprocessed_train')\n    train_path = os.path.join(base_path, folder_name, 'HF_Lung_V1/train')\n    \n    # Collecting all .npy files from preprocessed_train\n    if os.path.exists(preprocessed_train_path):\n        for npy_file in tqdm(os.listdir(preprocessed_train_path), desc=f\"Copying .npy files from {folder_name}\"):\n            if npy_file.endswith('.npy'):\n                npy_source = os.path.join(preprocessed_train_path, npy_file)\n                shutil.copy(npy_source, npy_destination)\n                npy_files_count += 1\n    \n    # Collecting all label files from train\n    if os.path.exists(train_path):\n        for label_file in tqdm(os.listdir(train_path), desc=f\"Copying label files from {folder_name}\"):\n            if label_file.endswith('_label.txt'):\n                label_source = os.path.join(train_path, label_file)\n                shutil.copy(label_source, label_destination)\n                label_files_count += 1\n\nnpy_destination_files = len(os.listdir(npy_destination))\nlabel_destination_files = len(os.listdir(label_destination))\n\nprint(f\"Expected .npy files: {npy_files_count}, Copied .npy files: {npy_destination_files}\")\nprint(f\"Expected label files: {label_files_count}, Copied label files: {label_destination_files}\")\n\nif npy_files_count == npy_destination_files:\n    print(\"All .npy files have been successfully copied.\")\nelse:\n    print(\"Warning: Some .npy files might be missing.\")\n\nif label_files_count == label_destination_files:\n    print(\"All label files have been successfully copied.\")\nelse:\n    print(\"Warning: Some label files might be missing.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:20:14.445440Z","iopub.execute_input":"2024-08-21T21:20:14.446720Z","iopub.status.idle":"2024-08-21T21:23:51.334929Z","shell.execute_reply.started":"2024-08-21T21:20:14.446673Z","shell.execute_reply":"2024-08-21T21:23:51.329759Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Copying .npy files from HF_Lung_V1-20240819T223709Z-001: 100%|██████████| 899/899 [00:18<00:00, 48.54it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-001: 100%|██████████| 3323/3323 [00:12<00:00, 273.19it/s]\nCopying .npy files from HF_Lung_V1-20240819T223709Z-002: 100%|██████████| 1594/1594 [00:33<00:00, 47.29it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-002: 100%|██████████| 3584/3584 [00:17<00:00, 208.41it/s]\nCopying .npy files from HF_Lung_V1-20240819T223709Z-003: 100%|██████████| 1671/1671 [00:41<00:00, 40.27it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-003: 100%|██████████| 2110/2110 [00:10<00:00, 198.92it/s]\nCopying .npy files from HF_Lung_V1-20240819T223709Z-004: 100%|██████████| 1619/1619 [00:36<00:00, 43.90it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-004: 100%|██████████| 131/131 [00:00<00:00, 309.35it/s]\nCopying .npy files from HF_Lung_V1-20240819T223709Z-005: 100%|██████████| 1564/1564 [00:33<00:00, 47.18it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-005: 100%|██████████| 761/761 [00:00<00:00, 941831.03it/s]\nCopying .npy files from HF_Lung_V1-20240819T223709Z-006: 100%|██████████| 462/462 [00:10<00:00, 42.52it/s]\nCopying label files from HF_Lung_V1-20240819T223709Z-006: 100%|██████████| 5709/5709 [00:00<00:00, 671752.27it/s]","output_type":"stream"},{"name":"stdout","text":"Expected .npy files: 7809, Copied .npy files: 7809\nExpected label files: 7809, Copied label files: 7809\nAll .npy files have been successfully copied.\nAll label files have been successfully copied.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Paths to the collected files\nnpy_destination = '/kaggle/working/collected_preprocessed_train'\nlabel_destination = '/kaggle/working/collected_labels'\n\n# Get lists of files in each directory\nnpy_files = sorted([f for f in os.listdir(npy_destination) if f.endswith('.npy')])\nlabel_files = sorted([f for f in os.listdir(label_destination) if f.endswith('_label.txt')])\n\n# Extract the base filenames (without extensions) for comparison\nnpy_ids = set([os.path.splitext(f)[0].replace('_processed', '') for f in npy_files])\nlabel_ids = set([os.path.splitext(f)[0].replace('_label', '') for f in label_files])\n\n# Find missing matches\nmissing_labels = npy_ids - label_ids\nmissing_npy = label_ids - npy_ids\n\n# Output results\nif not missing_labels and not missing_npy:\n    print(\"All .npy files have corresponding label files.\")\nelse:\n    if missing_labels:\n        print(f\"Missing labels for the following .npy files: {missing_labels}\")\n    if missing_npy:\n        print(f\"Missing .npy files for the following labels: {missing_npy}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:23:51.344252Z","iopub.execute_input":"2024-08-21T21:23:51.345909Z","iopub.status.idle":"2024-08-21T21:23:51.513508Z","shell.execute_reply.started":"2024-08-21T21:23:51.345727Z","shell.execute_reply":"2024-08-21T21:23:51.508466Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"All .npy files have corresponding label files.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### BiLSTM & BiGRU","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils as nn_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score, f1_score\n\n# Function to convert time string (hh:mm:ss.ms) to an index\ndef time_to_index(time_str, total_length, duration):\n    h, m, s = map(float, time_str.split(':'))\n    time_in_seconds = h * 3600 + m * 60 + s\n    index = int((time_in_seconds / duration) * total_length)\n    return index\n\n# Function to parse labels and convert them into binary vectors\ndef parse_label_file(label_file_path, sequence_length, duration):\n    label_vector = np.zeros(sequence_length)\n    with open(label_file_path, 'r') as file:\n        label_content = file.readlines()\n    for line in label_content:\n        match = re.match(r'(\\w) (\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})', line)\n        if match:\n            event, start_time, end_time = match.groups()\n            start_index = time_to_index(start_time, sequence_length, duration)\n            end_index = time_to_index(end_time, sequence_length, duration)\n            # Marking the segment for the event as 1 (presence of event)\n            label_vector[start_index:end_index] = 1\n    return label_vector\n\n# Custom dataset class for lung sound data\nclass LungSoundDataset(Dataset):\n    def __init__(self, npy_files, label_files, sequence_length=938, duration=15):\n        self.features = np.array([np.load(f) for f in npy_files])  # Convert list of arrays to a single array\n        self.labels = np.array([parse_label_file(f, sequence_length, duration) for f in label_files])\n\n        # Ensure the correct shape for the input (batch_size, sequence_length, feature_size)\n        self.features = torch.FloatTensor(self.features[:, :sequence_length, :]).transpose(1, 2)  # Transpose to [batch_size, sequence_length, feature_size]\n        self.labels = torch.FloatTensor(self.labels)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n# Bidirectional LSTM/GRU model definition\nclass BiRecurrentModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, rnn_type='BiLSTM'):\n        super(BiRecurrentModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        if rnn_type == 'BiLSTM':\n            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        elif rnn_type == 'BiGRU':\n            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        else:\n            raise ValueError(\"rnn_type must be 'BiLSTM' or 'BiGRU'\")\n        \n        self.fc1 = nn.Linear(hidden_size * 2, 32)  # hidden_size * 2 because of bidirection\n        self.fc2 = nn.Linear(32, 1)\n    \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n        if isinstance(self.rnn, nn.LSTM):\n            c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n            out, _ = self.rnn(x, (h0, c0))\n        else:\n            out, _ = self.rnn(x, h0)\n        \n        out = self.fc1(out)  \n        out = self.fc2(out)\n        out = torch.sigmoid(out)  \n        return out.squeeze(-1) \n\ndef check_and_fix_nan(tensor, name, default_value=0.0):\n    if torch.isnan(tensor).any():\n        print(f\"NaN detected in {name}, replacing with {default_value}\")\n        tensor = torch.where(torch.isnan(tensor), torch.tensor(default_value).to(tensor.device), tensor)\n    return tensor\n\ndef check_and_fix_bounds(tensor, name, min_val=0.0, max_val=1.0):\n    if (tensor < min_val).any() or (tensor > max_val).any():\n        print(f\"Out-of-bounds detected in {name}, clamping to [{min_val}, {max_val}]\")\n        tensor = torch.clamp(tensor, min=min_val, max=max_val)\n        tensor = torch.where(tensor < min_val, torch.tensor(min_val).to(tensor.device), tensor)\n        tensor = torch.where(tensor > max_val, torch.tensor(max_val).to(tensor.device), tensor)\n    return tensor\n\n# Function to train the model\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, fold, model_type):\n    model.to(device)\n    best_val_auc = 0.0\n    best_model_path = None\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, labels in tqdm(train_loader, desc=f\"{model_type} Epoch {epoch+1}/{num_epochs}\"):\n            features, labels = features.to(device), labels.to(device)\n            \n            features = check_and_fix_nan(features, \"features\")\n            features = check_and_fix_bounds(features, \"features\")\n            \n            labels = check_and_fix_nan(labels, \"labels\")\n            labels = check_and_fix_bounds(labels, \"labels\")\n\n            optimizer.zero_grad()\n            outputs = model(features)\n\n            outputs = check_and_fix_nan(outputs, \"raw model outputs\")\n            outputs = check_and_fix_bounds(outputs, \"raw model outputs\")\n\n            outputs = torch.sigmoid(outputs)\n\n            outputs = check_and_fix_nan(outputs, \"sigmoid outputs\")\n            outputs = check_and_fix_bounds(outputs, \"sigmoid outputs\")\n\n            loss = criterion(outputs, labels)\n\n            loss = check_and_fix_nan(loss, \"loss\")\n\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            for name, param in model.named_parameters():\n                if param.grad is not None:\n                    param.grad = check_and_fix_nan(param.grad, f\"gradient of {name}\")\n\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation step\n        model.eval()\n        val_loss = 0.0\n        all_labels = []\n        all_outputs = []\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.to(device), labels.to(device)\n\n                features = check_and_fix_nan(features, \"validation features\")\n                features = check_and_fix_bounds(features, \"validation features\")\n\n                labels = check_and_fix_nan(labels, \"validation labels\")\n                labels = check_and_fix_bounds(labels, \"validation labels\")\n                \n                outputs = model(features)\n\n                outputs = check_and_fix_nan(outputs, \"validation raw outputs\")\n                outputs = check_and_fix_bounds(outputs, \"validation raw outputs\")\n\n                outputs = torch.sigmoid(outputs)\n                outputs = check_and_fix_bounds(outputs, \"validation clamped outputs\")\n                \n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                all_labels.append(labels.cpu().numpy())\n                all_outputs.append(outputs.cpu().numpy())\n        \n        all_labels = np.concatenate(all_labels).flatten()\n        all_outputs = np.concatenate(all_outputs).flatten()\n        \n        val_auc = roc_auc_score(all_labels, all_outputs)\n        val_f1 = f1_score(all_labels, all_outputs > 0.5)\n        \n        print(f\"{model_type} Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n        print(f\"Val AUC: {val_auc:.4f}, Val F1: {val_f1:.4f}\")\n        \n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            best_model_path = f\"/kaggle/working/best_{model_type}_model_fold_{fold+1}_epoch_{epoch+1}.pth\"\n            torch.save(model.state_dict(), best_model_path)\n\n    return best_model_path\n\n# Cross-validation setup and training\ndef run_cross_validation(npy_files, label_files, input_size, hidden_size, num_layers, num_epochs, learning_rate, device):\n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n    best_models = {'BiLSTM': [], 'BiGRU': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(kfold.split(npy_files)):\n        print(f\"Training fold {fold+1}/{kfold.n_splits}\")\n        \n        train_npy_files = [npy_files[i] for i in train_idx]\n        val_npy_files = [npy_files[i] for i in val_idx]\n        train_label_files = [label_files[i] for i in train_idx]\n        val_label_files = [label_files[i] for i in val_idx]\n        \n        train_dataset = LungSoundDataset(train_npy_files, train_label_files)\n        val_dataset = LungSoundDataset(val_npy_files, val_label_files)\n        \n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n        val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4)\n        \n        # Training BiLSTM model\n        bilstm_model = BiRecurrentModel(input_size, hidden_size, num_layers, 'BiLSTM')\n        criterion = nn.BCELoss()\n        optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=learning_rate)\n        best_bilstm_model_path = train_model(bilstm_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, fold, 'BiLSTM')\n        best_models['BiLSTM'].append(best_bilstm_model_path)\n        \n        # Training BiGRU model\n        bigru_model = BiRecurrentModel(input_size, hidden_size, num_layers, 'BiGRU')\n        optimizer = torch.optim.Adam(bigru_model.parameters(), lr=learning_rate)\n        best_bigru_model_path = train_model(bigru_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, fold, 'BiGRU')\n        best_models['BiGRU'].append(best_bigru_model_path)\n    \n    print(f\"Best BiLSTM models from each fold: {best_models['BiLSTM']}\")\n    print(f\"Best BiGRU models from each fold: {best_models['BiGRU']}\")\n    return best_models\n\n# Paths to the preprocessed features and label files\nnpy_destination = '/kaggle/working/collected_preprocessed_train'\nlabel_destination = '/kaggle/working/collected_labels'\n\nnpy_files = sorted([os.path.join(npy_destination, f) for f in os.listdir(npy_destination) if f.endswith('.npy')])\nlabel_files = sorted([os.path.join(label_destination, f) for f in os.listdir(label_destination) if f.endswith('_label.txt')])\n\n# Parameters for model training\ninput_size = 193  # Feature dimension\nhidden_size = 128  # Hidden units in RNN cells\nnum_layers = 2  # Number of RNN layers\nnum_epochs = 5  # Number of epochs to train\nlearning_rate = 0.0001  # Learning rate\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Run cross-validation and train models\nbest_model_paths = run_cross_validation(npy_files, label_files, input_size, hidden_size, num_layers, num_epochs, learning_rate, device)\nprint(\"Training complete. Best models saved from each fold:\", best_model_paths)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:23:51.516252Z","iopub.execute_input":"2024-08-21T21:23:51.516751Z","iopub.status.idle":"2024-08-22T01:40:21.272606Z","shell.execute_reply.started":"2024-08-21T21:23:51.516710Z","shell.execute_reply":"2024-08-22T01:40:21.269418Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training fold 1/3\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5:  21%|██        | 34/163 [01:17<04:49,  2.24s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5: 100%|██████████| 163/163 [06:23<00:00,  2.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 1/5\nTrain Loss: 0.7073\nVal Loss: 0.6933\nVal AUC: 0.3965, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5:  88%|████████▊ | 144/163 [06:28<00:51,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5: 100%|██████████| 163/163 [07:20<00:00,  2.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 2/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.3934, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5:  80%|███████▉  | 130/163 [06:20<01:32,  2.80s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5: 100%|██████████| 163/163 [07:53<00:00,  2.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.3916, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5:   7%|▋         | 12/163 [00:34<07:01,  2.79s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5: 100%|██████████| 163/163 [07:54<00:00,  2.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.3908, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5:  58%|█████▊    | 94/163 [04:44<04:32,  3.95s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5: 100%|██████████| 163/163 [08:05<00:00,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.3904, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5:  33%|███▎      | 53/163 [02:29<05:03,  2.76s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5: 100%|██████████| 163/163 [07:41<00:00,  2.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 1/5\nTrain Loss: 0.7026\nVal Loss: 0.6935\nVal AUC: 0.4208, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5:  82%|████████▏ | 133/163 [06:37<01:27,  2.93s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5: 100%|██████████| 163/163 [08:05<00:00,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 2/5\nTrain Loss: 0.6933\nVal Loss: 0.6933\nVal AUC: 0.4207, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5:  29%|██▉       | 47/163 [02:18<05:34,  2.88s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5: 100%|██████████| 163/163 [07:49<00:00,  2.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4208, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5:  28%|██▊       | 45/163 [02:12<05:48,  2.96s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5: 100%|██████████| 163/163 [07:52<00:00,  2.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4209, Val F1: 0.5114\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5:  12%|█▏        | 20/163 [00:58<06:47,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5: 100%|██████████| 163/163 [07:48<00:00,  2.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4211, Val F1: 0.5114\nTraining fold 2/3\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5:  66%|██████▌   | 107/163 [02:57<01:39,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5: 100%|██████████| 163/163 [04:36<00:00,  1.69s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 1/5\nTrain Loss: 0.7111\nVal Loss: 0.6933\nVal AUC: 0.4118, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5:  32%|███▏      | 52/163 [01:37<03:18,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5: 100%|██████████| 163/163 [05:34<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 2/5\nTrain Loss: 0.6933\nVal Loss: 0.6932\nVal AUC: 0.4114, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5:   2%|▏         | 4/163 [00:12<07:21,  2.78s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5: 100%|██████████| 163/163 [07:06<00:00,  2.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4111, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5:  99%|█████████▉| 162/163 [08:50<00:03,  3.26s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5: 100%|██████████| 163/163 [08:53<00:00,  3.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4111, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5:   2%|▏         | 4/163 [00:20<12:44,  4.81s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5: 100%|██████████| 163/163 [09:52<00:00,  3.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiLSTM Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4117, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5:  51%|█████     | 83/163 [03:51<03:39,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5: 100%|██████████| 163/163 [07:44<00:00,  2.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 1/5\nTrain Loss: 0.7025\nVal Loss: 0.6934\nVal AUC: 0.4187, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5:  55%|█████▍    | 89/163 [04:09<03:14,  2.63s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5: 100%|██████████| 163/163 [07:26<00:00,  2.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 2/5\nTrain Loss: 0.6933\nVal Loss: 0.6932\nVal AUC: 0.4173, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5:  23%|██▎       | 38/163 [01:46<05:53,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5: 100%|██████████| 163/163 [07:31<00:00,  2.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4165, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5:  93%|█████████▎| 152/163 [07:04<00:31,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5: 100%|██████████| 163/163 [07:35<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4160, Val F1: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5:  56%|█████▋    | 92/163 [04:22<03:22,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5: 100%|██████████| 163/163 [07:57<00:00,  2.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"NaN detected in validation features, replacing with 0.0\nBiGRU Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4156, Val F1: 0.5160\nTraining fold 3/3\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5:  47%|████▋     | 76/163 [02:30<03:55,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5:  91%|█████████▏| 149/163 [05:08<00:31,  2.25s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 1/5: 100%|██████████| 163/163 [05:40<00:00,  2.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiLSTM Epoch 1/5\nTrain Loss: 0.7083\nVal Loss: 0.6933\nVal AUC: 0.4123, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5:  71%|███████   | 116/163 [04:34<01:45,  2.25s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5:  84%|████████▍ | 137/163 [05:20<00:55,  2.15s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 2/5: 100%|██████████| 163/163 [06:16<00:00,  2.31s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiLSTM Epoch 2/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4123, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5:  42%|████▏     | 69/163 [02:37<03:42,  2.36s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5:  79%|███████▊  | 128/163 [04:55<01:23,  2.37s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 3/5: 100%|██████████| 163/163 [06:29<00:00,  2.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiLSTM Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4123, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5:  20%|██        | 33/163 [01:22<05:11,  2.39s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5:  42%|████▏     | 69/163 [02:57<03:54,  2.49s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 4/5: 100%|██████████| 163/163 [07:00<00:00,  2.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiLSTM Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4119, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5:  21%|██▏       | 35/163 [01:42<05:31,  2.59s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5:  86%|████████▌ | 140/163 [06:26<01:02,  2.72s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiLSTM Epoch 5/5: 100%|██████████| 163/163 [07:29<00:00,  2.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiLSTM Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4115, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5:  16%|█▌        | 26/163 [01:13<06:15,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5:  45%|████▍     | 73/163 [03:23<04:05,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 1/5: 100%|██████████| 163/163 [07:35<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiGRU Epoch 1/5\nTrain Loss: 0.7040\nVal Loss: 0.6935\nVal AUC: 0.4120, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5:  48%|████▊     | 78/163 [03:38<03:57,  2.79s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5:  99%|█████████▉| 161/163 [07:29<00:05,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 2/5: 100%|██████████| 163/163 [07:34<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiGRU Epoch 2/5\nTrain Loss: 0.6933\nVal Loss: 0.6933\nVal AUC: 0.4113, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5:  37%|███▋      | 61/163 [02:51<04:42,  2.77s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 3/5: 100%|██████████| 163/163 [07:34<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiGRU Epoch 3/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4110, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5:  41%|████      | 67/163 [03:18<04:23,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5:  75%|███████▍  | 122/163 [05:50<01:51,  2.72s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 4/5: 100%|██████████| 163/163 [07:45<00:00,  2.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiGRU Epoch 4/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4109, Val F1: 0.5130\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5:   6%|▌         | 10/163 [00:29<07:03,  2.77s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5:  88%|████████▊ | 144/163 [06:40<00:52,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected in features, replacing with 0.0\n","output_type":"stream"},{"name":"stderr","text":"BiGRU Epoch 5/5: 100%|██████████| 163/163 [07:32<00:00,  2.78s/it]\n","output_type":"stream"},{"name":"stdout","text":"BiGRU Epoch 5/5\nTrain Loss: 0.6932\nVal Loss: 0.6932\nVal AUC: 0.4108, Val F1: 0.5130\nBest BiLSTM models from each fold: ['/kaggle/working/best_BiLSTM_model_fold_1_epoch_1.pth', '/kaggle/working/best_BiLSTM_model_fold_2_epoch_1.pth', '/kaggle/working/best_BiLSTM_model_fold_3_epoch_2.pth']\nBest BiGRU models from each fold: ['/kaggle/working/best_BiGRU_model_fold_1_epoch_5.pth', '/kaggle/working/best_BiGRU_model_fold_2_epoch_1.pth', '/kaggle/working/best_BiGRU_model_fold_3_epoch_1.pth']\nTraining complete. Best models saved from each fold: {'BiLSTM': ['/kaggle/working/best_BiLSTM_model_fold_1_epoch_1.pth', '/kaggle/working/best_BiLSTM_model_fold_2_epoch_1.pth', '/kaggle/working/best_BiLSTM_model_fold_3_epoch_2.pth'], 'BiGRU': ['/kaggle/working/best_BiGRU_model_fold_1_epoch_5.pth', '/kaggle/working/best_BiGRU_model_fold_2_epoch_1.pth', '/kaggle/working/best_BiGRU_model_fold_3_epoch_1.pth']}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### CNN added","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils as nn_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score, f1_score\n\n# Function to convert time string (hh:mm:ss.ms) to an index\ndef time_to_index(time_str, total_length, duration):\n    h, m, s = map(float, time_str.split(':'))\n    time_in_seconds = h * 3600 + m * 60 + s\n    index = int((time_in_seconds / duration) * total_length)\n    return index\n\n# Function to parse labels and convert them into binary vectors\ndef parse_label_file(label_file_path, sequence_length, duration):\n    label_vector = np.zeros(sequence_length)\n    with open(label_file_path, 'r') as file:\n        label_content = file.readlines()\n    for line in label_content:\n        match = re.match(r'(\\w) (\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})', line)\n        if match:\n            event, start_time, end_time = match.groups()\n            start_index = time_to_index(start_time, sequence_length, duration)\n            end_index = time_to_index(end_time, sequence_length, duration)\n            # Marking the segment for the event as 1 (presence of event)\n            label_vector[start_index:end_index] = 1\n    return label_vector\n\n# Custom dataset class for lung sound data\nclass LungSoundDataset(Dataset):\n    def __init__(self, npy_files, label_files, sequence_length=938, duration=15):\n        self.features = np.array([np.load(f) for f in npy_files])  # Convert list of arrays to a single array\n        self.labels = np.array([parse_label_file(f, sequence_length, duration) for f in label_files])\n\n        # Ensure the correct shape for the input (batch_size, sequence_length, feature_size)\n        self.features = torch.FloatTensor(self.features[:, :sequence_length, :]).transpose(1, 2)  # Transpose to [batch_size, sequence_length, feature_size]\n        self.labels = torch.FloatTensor(self.labels)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n# CNN + RNN model definition\nclass CNNRecurrentModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, rnn_type='LSTM', bidirectional=False):\n        super(CNNRecurrentModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.output_size = hidden_size * 2 if bidirectional else hidden_size\n\n        # CNN layers\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=(6, 6), padding=(2, 2))  # (N, 938, 193, 1) -> (N, 938, 193, 64)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=(4, 4), padding=(1, 1))  # (N, 938, 193, 64) -> (N, 469, 97, 64)\n        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n        self.bn2 = nn.BatchNorm2d(64)\n        self.dropout_cnn = nn.Dropout(0.3)\n\n        # Calculate the flattened size after CNN layers\n        self.flattened_size = self.calculate_flattened_size(input_size)\n\n        # RNN layers\n        if rnn_type == 'LSTM':\n            self.rnn = nn.LSTM(input_size=self.flattened_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n        elif rnn_type == 'GRU':\n            self.rnn = nn.GRU(input_size=self.flattened_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n        else:\n            raise ValueError(\"rnn_type must be 'LSTM' or 'GRU'\")\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(self.output_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n        self.dropout_rnn = nn.Dropout(0.3)\n\n    def calculate_flattened_size(self, input_size):\n        dummy_input = torch.zeros(1, 1, 938, input_size)\n        x = self.conv1(dummy_input)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.pool(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        flattened_size = x.view(x.size(0), x.size(2), -1).size(-1)\n        return flattened_size\n\n    def forward(self, x):\n        # CNN forward pass\n        x = x.unsqueeze(1)  # Add a channel dimension (N, 1, 938, 193)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.pool(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.dropout_cnn(x)\n        x = x.view(x.size(0), x.size(2), -1)  # Flatten the output\n\n        # RNN forward pass\n        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n        if isinstance(self.rnn, nn.LSTM):\n            c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n            out, _ = self.rnn(x, (h0, c0))\n        else:\n            out, _ = self.rnn(x, h0)\n\n        out = self.dropout_rnn(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        out = torch.sigmoid(out)\n        return out.squeeze(-1)\n\n# Function to check and fix NaN values\ndef check_and_fix_nan(tensor, name, default_value=0.0):\n    if torch.isnan(tensor).any():\n        print(f\"NaN detected in {name}, replacing with {default_value}\")\n        tensor = torch.where(torch.isnan(tensor), torch.tensor(default_value).to(tensor.device), tensor)\n    return tensor\n\n# Function to check and fix bounds\ndef check_and_fix_bounds(tensor, name, min_val=0.0, max_val=1.0):\n    if (tensor < min_val).any() or (tensor > max_val).any():\n        print(f\"Out-of-bounds detected in {name}, clamping to [{min_val}, {max_val}]\")\n        tensor = torch.clamp(tensor, min=min_val, max=max_val)\n        # Ensure strict bounds by forcefully setting values outside [0,1]\n        tensor = torch.where(tensor < min_val, torch.tensor(min_val).to(tensor.device), tensor)\n        tensor = torch.where(tensor > max_val, torch.tensor(max_val).to(tensor.device), tensor)\n    return tensor\n\n# Custom loss function to handle different sizes of outputs and labels\ndef custom_loss(outputs, labels):\n    if outputs.size(1) != labels.size(1):\n        labels = F.interpolate(labels.unsqueeze(1), size=outputs.size(1), mode='linear', align_corners=False).squeeze(1)\n    loss = nn.BCELoss()(outputs, labels)\n    return loss\n\n# Function to train the model\ndef train_model(model, train_loader, val_loader, optimizer, num_epochs, device, model_type):\n    model.to(device)\n    best_val_auc = 0.0\n    best_model_path = None\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, labels in tqdm(train_loader, desc=f\"{model_type} Epoch {epoch+1}/{num_epochs}\"):\n            features, labels = features.to(device), labels.to(device)\n            \n            # Check and fix NaN and bounds in features and labels\n            features = check_and_fix_nan(features, \"features\")\n            features = check_and_fix_bounds(features, \"features\")\n            \n            labels = check_and_fix_nan(labels, \"labels\")\n            labels = check_and_fix_bounds(labels, \"labels\")\n\n            optimizer.zero_grad()\n            outputs = model(features)\n\n            # Check and fix NaN and bounds in raw model outputs\n            outputs = check_and_fix_nan(outputs, \"raw model outputs\")\n            outputs = check_and_fix_bounds(outputs, \"raw model outputs\")\n\n            # Apply sigmoid activation to ensure outputs are between 0 and 1\n            outputs = torch.sigmoid(outputs)\n\n            # Check and fix NaN and bounds in sigmoid outputs\n            outputs = check_and_fix_nan(outputs, \"sigmoid outputs\")\n            outputs = check_and_fix_bounds(outputs, \"sigmoid outputs\")\n\n            loss = custom_loss(outputs, labels)\n\n            # Check and fix NaN in loss\n            loss = check_and_fix_nan(loss, \"loss\")\n\n            loss.backward()\n            \n            # Apply gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Check and fix NaN in gradients\n            for name, param in model.named_parameters():\n                if param.grad is not None:\n                    param.grad = check_and_fix_nan(param.grad, f\"gradient of {name}\")\n\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation step (similar NaN and bounds checks as above)\n        model.eval()\n        val_loss = 0.0\n        all_labels = []\n        all_outputs = []\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.to(device), labels.to(device)\n\n                # Check and fix NaN and bounds in features and labels during validation\n                features = check_and_fix_nan(features, \"validation features\")\n                features = check_and_fix_bounds(features, \"validation features\")\n\n                labels = check_and_fix_nan(labels, \"validation labels\")\n                labels = check_and_fix_bounds(labels, \"validation labels\")\n                \n                outputs = model(features)\n\n                # Check and fix NaN and bounds in raw outputs during validation\n                outputs = check_and_fix_nan(outputs, \"validation raw outputs\")\n                outputs = check_and_fix_bounds(outputs, \"validation raw outputs\")\n\n                outputs = torch.sigmoid(outputs)\n                outputs = check_and_fix_bounds(outputs, \"validation clamped outputs\")\n                \n                loss = custom_loss(outputs, labels)\n                val_loss += loss.item()\n                \n                all_labels.append(labels.cpu().numpy())\n                all_outputs.append(outputs.cpu().numpy())\n        \n        all_labels = np.concatenate(all_labels).flatten()\n        all_outputs = np.concatenate(all_outputs).flatten()\n        \n        val_auc = roc_auc_score(all_labels, all_outputs)\n        val_f1 = f1_score(all_labels, all_outputs > 0.5)\n        \n        print(f\"{model_type} Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n        print(f\"Val AUC: {val_auc:.4f}, Val F1: {val_f1:.4f}\")\n        \n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            best_model_path = f\"/kaggle/working/best_{model_type}_Model_epoch_{epoch+1}.pth\"\n            torch.save(model.state_dict(), best_model_path)\n\n    return best_model_path\n\n# Paths to the preprocessed features and label files\nnpy_destination = '/kaggle/working/collected_preprocessed_train'\nlabel_destination = '/kaggle/working/collected_labels'\n\nnpy_files = sorted([os.path.join(npy_destination, f) for f in os.listdir(npy_destination) if f.endswith('.npy')])\nlabel_files = sorted([os.path.join(label_destination, f) for f in os.listdir(label_destination) if f.endswith('_label.txt')])\n\n# Parameters for model training\ninput_size = 193  # Adjusted input size after CNN flattening\nhidden_size = 128  # Hidden units in RNN cells\nnum_layers = 2  # Number of RNN layers\nnum_epochs = 2  # Number of epochs to train\nlearning_rate = 0.0001  # Learning rate\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Prepare datasets and loaders\ntrain_dataset = LungSoundDataset(npy_files, label_files)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(train_dataset, batch_size=32, num_workers=4)  # Using train set as validation set for simplicity\n\n# Train CNN-LSTM model\ncnn_lstm_model = CNNRecurrentModel(input_size, hidden_size, num_layers, 'LSTM', bidirectional=False)\noptimizer = torch.optim.Adam(cnn_lstm_model.parameters(), lr=learning_rate)\nbest_cnn_lstm_model_path = train_model(cnn_lstm_model, train_loader, val_loader, optimizer, num_epochs, device, 'CNN_LSTM')\n\n# Train CNN-GRU model\ncnn_gru_model = CNNRecurrentModel(input_size, hidden_size, num_layers, 'GRU', bidirectional=False)\noptimizer = torch.optim.Adam(cnn_gru_model.parameters(), lr=learning_rate)\nbest_cnn_gru_model_path = train_model(cnn_gru_model, train_loader, val_loader, optimizer, num_epochs, device, 'CNN_GRU')\n\n# Train CNN-BiLSTM model\ncnn_bilstm_model = CNNRecurrentModel(input_size, hidden_size, num_layers, 'LSTM', bidirectional=True)\noptimizer = torch.optim.Adam(cnn_bilstm_model.parameters(), lr=learning_rate)\nbest_cnn_bilstm_model_path = train_model(cnn_bilstm_model, train_loader, val_loader, optimizer, num_epochs, device, 'CNN_BiLSTM')\n\n# Train CNN-BiGRU model\ncnn_bigru_model = CNNRecurrentModel(input_size, hidden_size, num_layers, 'GRU', bidirectional=True)\noptimizer = torch.optim.Adam(cnn_bigru_model.parameters(), lr=learning_rate)\nbest_cnn_bigru_model_path = train_model(cnn_bigru_model, train_loader, val_loader, optimizer, num_epochs, device, 'CNN_BiGRU')\n\nprint(f\"Best CNN-LSTM model path: {best_cnn_lstm_model_path}\")\nprint(f\"Best CNN-GRU model path: {best_cnn_gru_model_path}\")\nprint(f\"Best CNN-BiLSTM model path: {best_cnn_bilstm_model_path}\")\nprint(f\"Best CNN-BiGRU model path: {best_cnn_bigru_model_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T13:57:21.635818Z","iopub.execute_input":"2024-08-21T13:57:21.636288Z","iopub.status.idle":"2024-08-21T14:02:05.257016Z","shell.execute_reply.started":"2024-08-21T13:57:21.636252Z","shell.execute_reply":"2024-08-21T14:02:05.255307Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"CNN_LSTM Epoch 1/2:   2%|▏         | 4/245 [02:58<2:59:44, 44.75s/it]\n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}